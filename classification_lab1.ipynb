{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqhxaXiruiGLlM1HHBZ0rp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hisyamm13/Modul10/blob/master/classification_lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlYeJp_3utow"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community\n",
        "!pip install replicate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Replicate\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set the API token\n",
        "api_token = userdata.get('REPLICATE_API_TOKEN')\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = api_token\n",
        "\n",
        "# Model setup\n",
        "model = \"ibm-granite/granite-3.2-8b-instruct\"\n",
        "\n",
        "output = Replicate(\n",
        "    model=model,\n",
        "    replicate_api_token=api_token,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "_sS-6poyvmKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the customer reviews\n",
        "customer_reviews = [\n",
        " \"The battery lasts all day, and the performance is excellent.\",\n",
        " \"The screen is too dim outdoors, but I love the colors indoors.\",\n",
        " \"This phone is slow and keeps crashing when I open certain apps.\"\n",
        "]\n",
        "\n",
        "# Refine the prompt to include reviews\n",
        "reviews_text = \"\\n\".join([f\"Review {i+1}: {review}\" for i, review\n",
        "in enumerate(customer_reviews)])\n",
        "prompt = f\"\"\"\n",
        "Classify these reviews as Positive, Negative, or Mixed:\n",
        "{reviews_text}\n",
        "\"\"\"\n",
        "\n",
        "# Invoke the model with the example prompt\n",
        "response = output.invoke(prompt)\n",
        "# Print the response\n",
        "print(\"Granite Model Response:\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55T6cU9RyyfI",
        "outputId": "b1f15c08-3fac-4548-d6a3-f870275c6699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Granite Model Response:\n",
            "\n",
            "1. Positive - The review highlights two positive aspects of the product: the long battery life and excellent performance.\n",
            "\n",
            "2. Mixed - This review presents a mix of positive and negative feedback. While the reviewer appreciates the colors on the screen indoors, they find the screen to be too dim outdoors, indicating a drawback.\n",
            "\n",
            "3. Negative - This review is clearly negative. It mentions two significant issues with the product: slow performance and frequent crashes when opening certain apps.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define refined prompt\n",
        "refined_prompt = f\"\"\"\n",
        "Classify these reviews as positive, negative, or mixed, and tag\n",
        "relevant categories (battery life, screen quality, or\n",
        "performance):\n",
        "{reviews_text}\n",
        "\"\"\"\n",
        "\n",
        "# Invoke the model with the example prompt\n",
        "response = output.invoke(refined_prompt)\n",
        "\n",
        "# Print the response\n",
        "print(\"Granite Model Refined Response:\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa6sAgnR0Nl4",
        "outputId": "06921d19-cfa9-47bf-cfcd-40369839d7d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Granite Model Refined Response:\n",
            "\n",
            "1. Positive (Battery Life, Performance)\n",
            "2. Mixed (Screen Quality)\n",
            "3. Negative (Performance)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt to complete the task in 2 steps\n",
        "multitask_prompt = f\"\"\"\n",
        "Complete the task in 2 steps.\n",
        "Step 1: Classify these reviews as positive, negative, or mixed.\n",
        "Step 2: For each review, identify relevant categories: battery\n",
        "life, screen quality, or performance.\n",
        "{reviews_text}\n",
        "\"\"\"\n",
        "response = output.invoke(multitask_prompt)\n",
        "print(\"Granite Model Response:\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUzT-udg0cSH",
        "outputId": "8b3335c9-05c3-4e62-da85-4b05dce4606f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Granite Model Response:\n",
            "\n",
            "Step 1: Classification\n",
            "\n",
            "Review 1: Positive\n",
            "Review 2: Mixed\n",
            "Review 3: Negative\n",
            "\n",
            "Step 2: Categories\n",
            "\n",
            "Review 1:\n",
            "- Battery life: Positive (The battery lasts all day)\n",
            "- Performance: Positive (The performance is excellent)\n",
            "\n",
            "Review 2:\n",
            "- Screen quality: Mixed (The screen is too dim outdoors, but the colors are good indoors)\n",
            "\n",
            "Review 3:\n",
            "- Performance: Negative (This phone is slow and keeps crashing when I open certain apps)\n",
            "\n",
            "No explicit mention of battery life in Review 2 or 3, and no mention of screen quality in Review 3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the example to guide the model\n",
        "formatted_prompt = f\"\"\"\n",
        "Classify these reviews as Positive, Negative, or Mixed, and tag\n",
        "relevant categories. Use this format:\n",
        "- Sentiment: [Sentiment]\n",
        "- Categories: [Categories].\n",
        "{reviews_text}\n",
        "\"\"\"\n",
        "\n",
        "# Invoke the model with prompt\n",
        "response = output.invoke(formatted_prompt)\n",
        "\n",
        "# Print the response\n",
        "print(\"Granite Model Formatted Response:\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYlRsglJ1Bp8",
        "outputId": "0345cada-7ef4-4312-e49f-5fed8f3f1d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Granite Model Formatted Response:\n",
            "\n",
            "- Sentiment: Positive\n",
            "  Categories: Battery Life, Performance\n",
            "\n",
            "- Sentiment: Mixed\n",
            "  Categories: Screen Quality, Color Reproduction\n",
            "\n",
            "- Sentiment: Negative\n",
            "  Categories: Performance, App Stability\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_meetings = [\n",
        "\"\"\"\n",
        "The meeting began with a discussion of the Q3 marketing budget. It\n",
        "was decided that 40% of the budget will go to digital ads, 30% to\n",
        "events, and 30% to social media campaigns. The team emphasized the\n",
        "need for influencer partnerships to increase brand visibility and\n",
        "email marketing to boost direct engagement. A pilot program to\n",
        "test new ad formats will launch next month, with the team\n",
        "reviewing results by the end of Q3.\n",
        "Later, the team discussed campaign performance metrics. ROI\n",
        "monitoring will be a top priority, and adjustments will be made\n",
        "based on performance data.\n",
        "The events team raised concerns about resource allocation for\n",
        "upcoming trade shows, and it was agreed that an additional $10,000\n",
        "would be reallocated to cover these costs.\n",
        "Lastly, the team reviewed new creative concepts for the upcoming\n",
        "campaign, deciding to proceed with Concept 8, which tested better\n",
        "among focus groups. Deadlines for campaign assets were finalized:\n",
        "all deliverables must be submitted by July 15.\n",
        "\"\"\"\n",
        "]\n",
        "\n",
        "# Refine the prompt to include reviews\n",
        "reviews_text = \"\\n\".join([f\"Review {i+1}: {review}\" for i, review\n",
        "in enumerate(customer_meetings)])\n",
        "prompt = f\"\"\"\n",
        "Summarize this meeting:\n",
        "{reviews_text}\n",
        "\"\"\"\n",
        "\n",
        "# Invoke the model with example prompt\n",
        "response = output.invoke(prompt)\n",
        "\n",
        "# Print the response\n",
        "print(\"Granite Model Response:\\n\")\n",
        "print(response)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiIAt4hq2Lp_",
        "outputId": "acc77e7c-128e-4b19-b572-ec12c98be5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Granite Model Response:\n",
            "\n",
            "During the Q3 marketing budget review meeting, the team allocated 40% to digital ads, 30% to events, and 30% to social media campaigns. Emphasis was placed on influencer partnerships and email marketing. A pilot program for new ad formats will commence next month, with performance review by Q3 end. ROI monitoring will be a priority for campaign performance metrics, with adjustments made based on data. An additional $10,000 was reallocated to the events team for trade show costs. The team selected Concept 8 for the upcoming campaign, based on focus group feedback, with all assets due by July 15.\n",
            "Granite Model Refined Response:\n",
            "\n",
            "During the Q3 marketing budget review, it was decided to allocate 40% to digital ads, 30% to events, and 30% to social media campaigns, with an emphasis on influencer partnerships and email marketing. A pilot program for new ad formats will launch next month, and ROI monitoring will be a priority for campaign performance metrics. The team agreed to reallocate an additional $10,000 for trade show resources and chose Concept 8 for the upcoming campaign, setting a final deadline of July 15 for all deliverables.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define refined prompt\n",
        "refined_prompt = f\"\"\"\n",
        "Summarize this meeting in three sentences:\n",
        "{reviews_text}\n",
        "\"\"\"\n",
        "\n",
        "# Invoke the model with refined prompt\n",
        "response = output.invoke(refined_prompt)\n",
        "\n",
        "# Print the response\n",
        "print(\"Granite Model Refined Response:\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLatCtxV22jh",
        "outputId": "628e9aac-40e6-45bc-eb26-3dcb2e447a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Granite Model Refined Response:\n",
            "\n",
            "During the Q3 marketing budget review, it was decided to allocate 40% to digital ads, 30% to events, and 30% to social media campaigns, with a focus on influencer partnerships and email marketing. The team also agreed to launch a pilot program for new ad formats, monitor ROI closely, and allocate an additional $10,000 for trade show resources. Lastly, Concept 8 was chosen for the upcoming campaign based on focus group results, with all assets due by July 15.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# classification Lab 2"
      ],
      "metadata": {
        "id": "eKM-AW9t70xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the customer reviews\n",
        "customer_reviews = [\n",
        " \"The battery lasts all day, but the phone gets hot during gaming.\",\n",
        " \"The screen is too dim outdoors, but I love the colors indoors.\",\n",
        " \"This phone is fast, but it keeps crashing when I open certain apps.\"\n",
        "]\n",
        "\n",
        "# Refine the prompt to include reviews\n",
        "reviews_text = \"\\n\".join([f\"Review {i+1}: {review}\" for i,\n",
        "review in enumerate(customer_reviews)])"
      ],
      "metadata": {
        "id": "uNkYiLxl7649"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model parameters for prompting with default values\n",
        "parameters = {\n",
        " \"top_k\": 0,\n",
        " \"top_p\": 1.0,\n",
        " \"max_tokens\": 256,\n",
        " \"min_tokens\": 0,\n",
        " \"random_seed\": None,\n",
        " \"repetition_penalty\": 1.0,\n",
        " \"stopping_criteria\": \"length (256 tokens)\",\n",
        " \"stopping_sequence\": None\n",
        "}"
      ],
      "metadata": {
        "id": "pNa-Nkoo8OL1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add initial prompt\n",
        "refined_prompt = f\"\"\":\n",
        "Classify these reviews as positive, negative, or mixed, and tag\n",
        "relevant focus areas such as battery life, screen quality, or\n",
        "performance\n",
        "{reviews_text}\n",
        "\"\"\"\n",
        "\n",
        "# Invoke the model\n",
        "response = output.invoke(refined_prompt, parameters=parameters)\n",
        "\n",
        "# Print the response\n",
        "print(\"Granite Model Refined Response:\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmXkPGF_8UoN",
        "outputId": "049ac28f-0b9f-41a1-866e-8dc2cccf0641"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Granite Model Refined Response:\n",
            "\n",
            "1. Positive (Battery Life), Negative (Performance/Heat Management)\n",
            "2. Mixed (Screen Quality), Negative (Outdoor Visibility)\n",
            "3. Positive (Performance), Negative (Stability/App Compatibility)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Refine multiple Model Parameter values\n",
        "parameters = {\n",
        " \"top_k\": 1,\n",
        " \"top_p\": 0.5,\n",
        " \"max_tokens\": 3,\n",
        " \"min_tokens\": 1,\n",
        " \"random_seed\": None,\n",
        " \"repetition_penalty\": 1.5,\n",
        " \"stopping_criteria\": \"length\",\n",
        " \"stopping_sequence\": \" \"\n",
        "}"
      ],
      "metadata": {
        "id": "wiLYN1Dp-nAE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add initial prompt\n",
        "refined_prompt = f\"\"\":\n",
        "Classify these reviews as positive, negative, or mixed, and tag\n",
        "relevant focus areas such as battery life, screen quality, or\n",
        "performance\n",
        "{reviews_text}\n",
        "\"\"\"\n",
        "\n",
        "# Invoke the model\n",
        "response = output.invoke(refined_prompt, parameters=parameters)\n",
        "\n",
        "# Print the response\n",
        "print(\"Granite Model Refined Response:\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLxRUr42-2or",
        "outputId": "e96e3c32-4676-4b94-afac-c50517d17b2f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Granite Model Refined Response:\n",
            "\n",
            "1. Positive (Battery Life), Negative (Heat during gaming, Performance)\n",
            "2. Mixed (Screen Quality), Negative (Outdoor visibility)\n",
            "3. Positive (Performance), Negative (App crashing)\n"
          ]
        }
      ]
    }
  ]
}